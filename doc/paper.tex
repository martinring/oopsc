\documentclass{style}

\begin{document}

\maketitle

%=======================================================================================

This document describes a compiler for the OOPS programming language. OOPS is a `fictional' language introduced in the course ``Ãœbersetzer'' (Compiler) at the University of Bremen. No language specification exists, but the language is defined through an existing reference compiler written in Java.

In the course students are required to extend the reference compiler with several features. The scala based compiler offers an alternative base, on which these features can be implemented, which utilizes functional concepts like parser combinators, algebraic data structures and transformation monads.      
 
\section{Lexical Analysis}

Because we want to use the parser combinators included in the standard Scala library we need to build a lexer that is compatible with the parser combinators. Luckyly this is very easy. The only thing we need to do is to use a lexer based on the \texttt{Scanner} class which is also included in the Scala standard Library. 

First we define our tokens in \texttt{de.martinring.oopsc.parsing.Tokens.scala}:

\begin{lstlisting}[name=tokens]
/*
 * Defines the tokens that are distinguished in the OOPS language.
 */
trait OOPSTokens extends Tokens {
  /* Represents a keyword (Reserved word or delimiter) */
  case class Keyword(chars: String) extends Token  {
    override def toString = Lexical.delimiters(chars)
  }
  /* Represents a number literal */
  case class Number(chars: String) extends Token {
    require(!chars.exists(x => !('0' to '9').contains(x)))
    val value = chars.toInt
    override def toString = "NUMBER " + chars
  }
  /* Represents an identifier */
  case class Identifier(chars: String) extends Token {
    override def toString = "IDENTIFIER " + chars
  }
}
\end{lstlisting}

That is pretty straight forward and self explaining. Next thing we do is define a Scanner which processes a \texttt{Source} and produces a stream of tokens that can be consumed by our parser. So we define an object \texttt{Lexical}

\begin{lstlisting}[name=lexical]
package de.martinring.oopsc.parsing

import scala.util.parsing.combinator.lexical.Scanners
import scala.util.parsing.input.CharArrayReader.EofCh

object Lexical extends Scanners with OOPSTokens {
\end{lstlisting}

and define reserved words and delimiters:

\begin{lstlisting}[name=lexical]
  /* set of reserved words which are parsed as keyword tokens */
  val reserved = Set(
    "CLASS", "IS", "END", "METHOD", "BEGIN", "READ", "NEW",
    "WRITE", "IF", "THEN", "WHILE", "DO", "MOD", "SELF")

  /* set of delimiter symbols which are parsed as keyword tokens */
  val delimiters = Map(
      "#" -> "NOT EQUAL",
      "(" -> "OPENING PARENTHESES",
      ")" -> "CLOSING PARENTHESES",
      "*" -> "TIMES",
      "+" -> "PLUS",
      "-" -> "MINUS",
      "." -> "ACCESS",
      "/" -> "DIVIDE",
      ":=" -> "ASSIGN",
      ":" -> "TYPE OF",
      ";" -> "SEMICOLON",
      "<=" -> "LESS OR EQUAL",
      "<" -> "LESS",
      "=" -> "EQUAL",
      ">=" -> "GREATER OR EQUAL",
      ">" -> "GREATER") withDefault identity

\end{lstlisting}

next we define parsers for white space, letters and digits.

\begin{lstlisting}[name=lexical]
  /* parses whitespace and comments and drops them */
  def whitespace: Parser[Any] =
    ( whitespaceChar
    | '{' ~ allExcept(EofCh) ~ '}'
    | '|' ~ rep(allExcept(EofCh, '\n'))
    | '{' ~ failure("unclosed comment") )*

  /* parses characters A-Z and a-z */
  val letter = elem("letter", ch => 'A' <= ch && ch <= 'Z' || 'a' <= ch && ch <= 'z')

  /* parses characters 0-9 */
  val digit = elem("digit", ch => '0' <= ch && ch <= '9')

  /* parses all characters between 0x00 and 0x20 except EOT (0x04) */
  val whitespaceChar = elem("space char", ch => ch <= ' ' && ch != EofCh)

  /* decides if a sequence of chars is an identifier or a keyword by looking up if it is contained in the list
  of reserved words */
  def identifierOrKeyword(name: String) =
    if (reserved contains name) Keyword(name) else Identifier(name)
\end{lstlisting}

last but not least we need to define a combined parser, that can parse a \texttt{Token}. This is our lexer and we can now process OOPS files and get a stream of tokens:

\begin{lstlisting}[name=lexical]
  /* parses a single token which can be an identifier, a keyword or a number */
  def token: Parser[Token] =
    ( letter ~ (letter|digit*)     ^^ { case first ~ rest => identifierOrKeyword(first :: rest mkString "") }
    | (digit+)                     ^^ { case digits => Number(digits mkString "") }
    | EofCh                        ^^^  EOF
    | delim
    | failure("illegal character") )
\end{lstlisting}

\section{Syntactical Analysis}

The syntactical analysis requires us to create an AST. That is very straight forward, because the abstract syntax is given. So all we need to do is to translate it into algebraic types: (this is from \texttt{de.martinring.oopsc.ast})

\begin{lstlisting}[name=syntax]
trait Element extends Positional
  
case class Program(main: Class) extends Element
  
trait Declaration extends Element { val name: String }
  
case class Class(name: String, members: List[Member] = Nil) extends Declaration  

trait Member extends Declaration
case class Attribute(name: String, typed: Name) extends Member
case class Method(name: String, 
                  variables: List[Variable], 
                  body: List[Statement]) extends Member
case class Variable(name: String, typed: Name) extends Declaration
  
trait Statement extends Element
case class Read(operand: Expression) extends Statement
case class Write(operand: Expression) extends Statement
case class While(condition: Expression, body: List[Statement]) extends Statement
case class If(condition: Expression, body: List[Statement]) extends Statement
case class Call(call: Expression) extends Statement
case class Assign(left: Expression, right: Expression) extends Statement

trait Expression extends Element
case class Unary(operator: String, operand: Expression) extends Expression
case class Binary(operator: String, left: Expression, right: Expression) extends Expression
case class Literal(value: Int, typed: String) extends Expression
case class New(typed: String) extends Expression
case class Access(left: Expression, right: Name) extends Expression
case class Name(name: String) extends Expression    
\end{lstlisting}

All nodes of the AST derrive from \texttt{Element} which mixes in the \texttt{Positional} trait from the Scala standard library to store position information.

there are four types of elements: \texttt{Program}s, \texttt{Declaration}s, \texttt{Statement}s and \texttt{Expression} which are all represented by traits. \texttt{Declaratin} has a subtrait \texttt{Member} which is implemented by \texttt{Attribute} and \texttt{Method} because these two can be a member of a class unlike \texttt{Variable} wich derrives directly from \texttt{Declaration}. The rest of the AST is so close to the Java equivalent that it needs no further explanation.

So now, that we have our AST defined we need to build a parser, that consumes the token stream our scanner produces to produce an AST itself. To achieve this we utilize \texttt{scala.util.parsing.combinator.syntactical.TokenParsers}. First we need to rewrite our syntax using parser combinators (\texttt{de.martinring.oopsc.parsing}).

\lstset{belowskip=5pt}
\begin{lstlisting}[name=parser]
def program = 
    classdecl                                           
\end{lstlisting}
\lstset{aboveskip=5pt}
\begin{lstlisting}[name=parser]
def classdecl =
    "CLASS" ~> name ~ "IS" ~
    ( memberdecl * ) <~
    "END" <~ "CLASS"     
\end{lstlisting}
\begin{lstlisting}[name=parser]
def memberdecl =
  ( attribute <~ ";"
  | method                                              
\end{lstlisting}
\begin{lstlisting}[name=parser]
def method =
    "METHOD" ~> name ~ "IS" ~
    ( variable <~ ";" *) ~
    "BEGIN" ~ (statement*) <~
    "END" <~ "METHOD"                                   
\end{lstlisting}
\begin{lstlisting}[name=parser]
def attribute =
    rep1sep( name, "," ) ~ ":" ~ name                   
\end{lstlisting}
\begin{lstlisting}[name=parser]
def variable =
    rep1sep( name, "," ) ~ ":" ~ name                   
\end{lstlisting}
\begin{lstlisting}[name=parser]
def statement =
    "READ" ~> memberaccess <~ ";"                       
  | "WRITE" ~> expr <~ ";"                              
  | "IF" ~> relation ~
    "THEN" ~ (statement*) <~
    "END" <~ "IF"                                       
  | "WHILE" ~> relation ~
    "DO" ~ (statement*) <~
    "END" <~ "WHILE"                                    
  | memberaccess <~ ";"                                 
  | memberaccess ~ ":=" ~ expr <~ ";"                   
  | failure ("illegal start of statement"))
\end{lstlisting}
\begin{lstlisting}[name=parser]
def relation =
    expr ~ (("="|"#"|"<"|">"|"<="|">=") ~ expr *)
\end{lstlisting}
\begin{lstlisting}[name=parser]
def expr =
    term ~ (("+"|"-") ~ term *)                         
\end{lstlisting}
\begin{lstlisting}[name=parser]
def term = 
    factor ~ (("*"|"/"|"MOD") ~ factor *)               
\end{lstlisting}
\begin{lstlisting}[name=parser]
def factor = 
    "-" ~ factor                                        
  | memberaccess )
\end{lstlisting}
\begin{lstlisting}[name=parser]
def memberaccess =
    literal ~ ("." ~ name *)                            
\end{lstlisting}
\begin{lstlisting}[name=parser]
def literal = 
    number                                              
  | "NULL"                                              
  | "SELF"                                              
  | "NEW" ~> name                                       
  | "(" ~> expr <~ ")"
  | name ) 
\end{lstlisting}

the used combinators are for concatenation, to ignore a result, \texttt{*} for zero or more occurrences and \texttt{+} for one or more.

To write keywords as strings as we did, we use a trick and define an implicit function that converts a string into a keyword parser:

\begin{lstlisting}[name=parser]
implicit def keyword(k: String) = accept(Keyword(k))
\end{lstlisting}

The problem with all this is, that it produces cryptic data structures. And we actually want to use our AST that we defined before. So we need to define what the parsers shall produce. We can do that with pattern matching on the structures and converting it to our AST. For example classes could be parsed like that:

\begin{lstlisting}[name=parser]
  def classdecl: Parser[Class] =
      "CLASS" ~> name ~ "IS" ~
      ( memberdecl * ) <~
      "END" <~ "CLASS"                                    
      { case id~ ~ms => Class(id.name, ms.flatten) at id }
\end{lstlisting}

we also need to wrap every parser with the \texttt{positional} parser which preserves the position information from the scanner and writes it to our \texttt{Element}s.

For the full parser take a look at \texttt{Parser.scala}

\section{Context Analysis}

For the context analysis we need to refine our AST a little bit. Such as including type information in Expressions, offsets in Variables and introduce types for boxing, unboxing and dereferencing expressions. There is no magic to all that so we leave it at that and don't waste paper with that code.

The concept for the context analysis is to use a monad and the \texttt{for} blocks (Which are the scala equivalent to haskells do notation). The monad shall carry a declaration table as state and be able to collect errors on the go with or without failing. Unfortunately the scala type inference is not as strong as the type inference of Haskell which is why some experiments with monad transformers failed. So we need to build one big monad to rule them all. We call it the \texttt{Transform} monad:

\begin{lstlisting}
trait Transform[A] {
  import Transform.

  def apply(context: Context = Context(Declarations())): Failable[(Context, A)]

  def map[B](f: A => B): Transform[B] = transform(
    apply( ).map{ case (c,a) => (c,f(a)) })

  def flatMap[B](f: A => Transform[B]): Transform[B] = transform(
    apply( ).flatMap { case (c,a) => f(a)(c) })
}
\end{lstlisting}

the \texttt{flatMap} function here is the Scala equivalent to Haskells bind operation.

This utilizes the \texttt{Declarations} type which is a declaration tree, that gets updated on the go. All the operations defined with the \texttt{Transform} monad can be found in the companion object in the same file. (\texttt{Transform.scala})

with operations like \texttt{enter} and \texttt{leave} to enter or leave a scope, \texttt{bind} and \texttt{rebind} to bind variables in scope, \texttt{resolve*} and so on we can write the context analysis pretty declarative. For example:

\begin{lstlisting}
    case a: Assign => for {
      left  <- expression(a.left)
            <- require(left.lvalue) (Error(left.pos, "l-value expected"))
      right <- expression(a.right) & box & requireType(left.typed)
    } yield Assign(left, right) at a    
\end{lstlisting}

What this does is: It does the context analysis for the left side of the assignment (2), checks if it is an lvalue (3) and then does the context analysis of the right side, boxes it if required and chechs the type. (4)

Let's look at another example:

\begin{lstlisting}
  def method(m: Method): Transform[Method] = for {
              <- enter(m, m.variables)    
    self      <- currentType
              <- bind(Variable("SELF", self.name, -2))
              <- incOffset(1) // skip return
    variables <- sequence(m.variables map variable)
              <- rebind(variables)
    body      <- sequence(m.body map statement)
              <- leave
  } yield Method(m.name, variables, body)
\end{lstlisting}

here we enter a new scope and bind the variables of the method (2). Then we determine the current type (3) and bind a variable \texttt{SELF} of this type. (4). We increase the offset by one (5) and then sequentially process all variables (6). afterwards we need to rebind the variables to our state because they are now annotated with further information. (7) Next we sequentially process the body of the method (8) and finally leave the current scope again. (9) We yield a result that contains the annotated variables and body.

For the entire context analysis see \texttt{ContextAnalysis.scala}

\section{Code Generation}

The code generation also utilizes the \texttt{Transform} monad to resolve types and so on. Which means, that we still need to explicitly enter and leave scopes when generating code for classes or methods. In \texttt{Assembler.scala} we defined a DSL for the Assembler code that we can use here. This is just playing around with the DSL-capabilities of scala and doesn't need to be thoroughly understood.

Again, let's illustrate with an example

\begin{lstlisting}
  case Program(main) => for {
    mainClass <- generate(main)
    main      <- generate(Access(New("Main"), "main"))      
  } yield lines(
    "; Start-Code: initialize registers",
    R(1) := 1       |> "R1 is allways 1",
    R(2) := stack   |> "R2 points to stack",
    R(4) := heap    |> "R4 points to next free position on Heap",
    main,
    R(0) := end     |> "exit program",
    mainClass,
    stack + ":"     |> "Start of stack",
    "DAT " + App.stackSize + ", 0;",
    heap + ":"      |> "Start of heap",
    "DAT " + App.heapSize + ", 0;",
    end + ":"       |> "End of program" )
\end{lstlisting}

this is pretty straight forward and only copied from the java implementation. For the full source look at \texttt{Code.scala}

\section{Summing up}

I think the biggest structural advantage of the Scala code is, that it is very aspect oriented. Which means there is a file that defines the plain AST types, a file that defines the syntax, a file for the context analysis and so on. In a Java-like approach all that is mixed up and spread across all the AST files which can get very confusing.

Writing a whole new compiler was a big load of work. And can not be reccommended to anyone who wants to get the quick results. But I learned very much about Scala and its limits during the implementation. So I dont regret my decision.

If I had to do it again I would use mutable data for the annotations during context analysis. But it was an interesting experiment to see just how functional it could get. ;) 

\end{document}